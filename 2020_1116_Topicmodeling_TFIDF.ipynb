{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from math import log\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samsung\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(r\"C:\\Users\\Samsung\\Desktop\\Project\\이희정 교수님\\project\\data\\result\\result_long\\most_long\")\n",
    "file_list_xlsx = [f for f in file_list if f.endswith(\".xlsx\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "types_ = [\"_Adj.xlsx\",\"_Adverb.xlsx\",\"_All.xlsx\",\"_ETC.xlsx\",\"_Noun.xlsx\",\"_Unknown.xlsx\",\"_Verb.xlsx\"]\n",
    "Adj= [f for f in file_list_xlsx if f.endswith(types_[0])]\n",
    "Adverb = [f for f in file_list_xlsx if f.endswith(types_[1])]\n",
    "All = [f for f in file_list_xlsx if f.endswith(types_[2])]\n",
    "ETC = [f for f in file_list_xlsx if f.endswith(types_[3])]\n",
    "Noun = [f for f in file_list_xlsx if f.endswith(types_[4])]\n",
    "Unknown = [f for f in file_list_xlsx if f.endswith(types_[5])]\n",
    "Verb = [f for f in file_list_xlsx if f.endswith(types_[6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'맨 케어_Adj.xlsx'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta_ = file_list[1]\n",
    "ta_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ta_):\n",
    "    path_ = r\"C:\\Users\\Samsung\\Desktop\\Project\\이희정 교수님\\project\\data\\result\\result_long\\most_long\\{}\"\n",
    "    data_ = pd.read_excel(path_.format(ta_),encoding='cp949').drop(columns=\"Unnamed: 0\")\n",
    "    return data_\n",
    "\n",
    "def load_data_v2(ta_,path_):\n",
    "    data_ = pd.read_excel(path_.format(ta_),encoding='cp949').drop(columns=\"Unnamed: 0\")\n",
    "    return data_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3247884317b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data_' is not defined"
     ]
    }
   ],
   "source": [
    "data_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_moding(k,documents):\n",
    "    K=k\n",
    "    random.seed(0)\n",
    "    D = len(documents)\n",
    "    def p_topic_given_document(topic, d, alpha=0.1):\n",
    "        return ((document_topic_counts[d][topic] + alpha) /(document_lengths[d] + K * alpha))\n",
    "\n",
    "    def p_word_given_topic(word, topic, beta=0.1):\n",
    "        return ((topic_word_counts[topic][word] + beta) /(topic_counts[topic] + V * beta))\n",
    "\n",
    "    def topic_weight(d, word, k):\n",
    "        return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "    def choose_new_topic(d, word):\n",
    "        return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "    def sample_from(weights):\n",
    "        total = sum(weights)\n",
    "        rnd = total * random.random()\n",
    "        for i, w in enumerate(weights):\n",
    "            rnd -= w\n",
    "            if rnd <= 0:\n",
    "                return i\n",
    "\n",
    "    document_topics = [[random.randrange(K) for word in document] for document in documents]\n",
    "    document_topic_counts = [Counter() for _ in documents]\n",
    "    topic_word_counts = [Counter() for _ in range(K)]\n",
    "    topic_counts = [0 for _ in range(K)]\n",
    "    document_lengths = list(map(len, documents))\n",
    "    distinct_words = set(word for document in documents for word in document)\n",
    "    V = len(distinct_words)\n",
    "\n",
    "    D = len(documents)\n",
    "    for d in range(D):\n",
    "        for word, topic in zip(documents[d], document_topics[d]):\n",
    "            document_topic_counts[d][topic] += 1\n",
    "            topic_word_counts[topic][word] += 1\n",
    "            topic_counts[topic] += 1\n",
    "\n",
    "\n",
    "    for d in range(D):\n",
    "        for word, topic in zip(documents[d], document_topics[d]):\n",
    "            document_topic_counts[d][topic] += 1\n",
    "            topic_word_counts[topic][word] += 1\n",
    "            topic_counts[topic] += 1\n",
    "    return document_topic_counts, topic_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6f1495a2f1a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"setence_tag\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data_' is not defined"
     ]
    }
   ],
   "source": [
    "data_.loc[5,\"setence_tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i2 in range(len(file_list_xlsx)):\n",
    "    ta_ = file_list_xlsx[i2]\n",
    "    read_path_ = r\"C:\\Users\\Samsung\\Desktop\\Project\\이희정 교수님\\project\\data\\result\\result_long\\most_long\\{}\"\n",
    "    save_path_ = r\"C:\\Users\\Samsung\\Desktop\\Project\\이희정 교수님\\project\\data\\result\\result_long\\most_long\\most_topic\\LDA_{}\"\n",
    "    data_ = pd.read_excel(read_path_.format(ta_),encoding='cp949').drop(columns=\"Unnamed: 0\")\n",
    "    k = 1\n",
    "    window = 8\n",
    "    Topic_2 = []\n",
    "    for num_ in data_.index:\n",
    "        list_ = ast.literal_eval(data_.loc[num_,\"setence_tag\"])\n",
    "        word_ = data_.loc[num_,\"word\"]\n",
    "        #print(word_)\n",
    "        try:\n",
    "            removal = [li.remove(word_) for li in list_]\n",
    "        except:\n",
    "            pass\n",
    "        docu_,topic_words = topic_moding(k,list_)\n",
    "        top = topic_words[0].most_common(window)\n",
    "        Topic_2.append(top)\n",
    "    data_[\"Topic\"] = Topic_2\n",
    "    data_.to_excel(save_path_.format(ta_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling_edit():\n",
    "    for i2 in range(len(file_list_xlsx)):\n",
    "        ta_ = file_list_xlsx[i2]\n",
    "        read_path_ = r\"C:\\Users\\Samsung\\Desktop\\Project\\이희정 교수님\\project\\data\\result\\result_long\\most_long\\{}\"\n",
    "        save_path_ = r\"C:\\Users\\Samsung\\Desktop\\Project\\이희정 교수님\\project\\data\\result\\result_long\\most_long\\most_topic\\LDA_{}\"\n",
    "        data_ = pd.read_excel(read_path_.format(ta_),encoding='cp949').drop(columns=\"Unnamed: 0\")\n",
    "        k = 1\n",
    "        window = 8\n",
    "        Topic_2 = []\n",
    "        for num_ in data_.index:\n",
    "            list_ = ast.literal_eval(data_.loc[num_,\"setence_tag\"])\n",
    "            word_ = data_.loc[num_,\"word\"]\n",
    "            #print(word_)\n",
    "            try:\n",
    "                removal = [li.remove(word_) for li in list_]\n",
    "            except:\n",
    "                pass\n",
    "            docu_,topic_words = topic_moding(k,list_)\n",
    "            top = topic_words[0].most_common(window)\n",
    "            Topic_2.append(top)\n",
    "        data_[\"Topic\"] = Topic_2\n",
    "        data_.to_excel(save_path_.format(ta_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dic = {\n",
    "    \"Adj\":Adj,\n",
    "    \"Adverb\":Adverb,\n",
    "    \"All\":All,\n",
    "    \"ETC\":ETC,\n",
    "    \"Noun\":Noun,\n",
    "    \"Unknown\":Unknown,\n",
    "    \"Verb\":Verb,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['맨 케어_Adj.xlsx',\n",
       " '베이스 메이크업_Adj.xlsx',\n",
       " '보디 케어_Adj.xlsx',\n",
       " '선 케어_Adj.xlsx',\n",
       " '스킨 케어_Adj.xlsx',\n",
       " '스타일러_Adj.xlsx',\n",
       " '이너 뷰티_Adj.xlsx',\n",
       " '컬러 메이크업_Adj.xlsx',\n",
       " '프래그런스_Adj.xlsx',\n",
       " '헤어 케어_Adj.xlsx']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adj'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_ = list(tf_dic.keys())[0]\n",
    "key_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['맨 케어_Adj.xlsx',\n",
       " '베이스 메이크업_Adj.xlsx',\n",
       " '보디 케어_Adj.xlsx',\n",
       " '선 케어_Adj.xlsx',\n",
       " '스킨 케어_Adj.xlsx',\n",
       " '스타일러_Adj.xlsx',\n",
       " '이너 뷰티_Adj.xlsx',\n",
       " '컬러 메이크업_Adj.xlsx',\n",
       " '프래그런스_Adj.xlsx',\n",
       " '헤어 케어_Adj.xlsx']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_dic[key_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic = {}\n",
    "for file in tf_dic[key_]:\n",
    "    #print(file)\n",
    "    df_ = load_data(file).drop(columns=[\"setence_tag\",\"setence\",\"word2vec\"])\n",
    "    df_dic[file] = df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['맨 케어_Adj.xlsx',\n",
       " '베이스 메이크업_Adj.xlsx',\n",
       " '보디 케어_Adj.xlsx',\n",
       " '선 케어_Adj.xlsx',\n",
       " '스킨 케어_Adj.xlsx',\n",
       " '스타일러_Adj.xlsx',\n",
       " '이너 뷰티_Adj.xlsx',\n",
       " '컬러 메이크업_Adj.xlsx',\n",
       " '프래그런스_Adj.xlsx',\n",
       " '헤어 케어_Adj.xlsx']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['촉촉',\n",
       " '산뜻',\n",
       " '깔끔',\n",
       " '민감',\n",
       " '예민',\n",
       " '시원',\n",
       " '간편',\n",
       " '강력',\n",
       " '칙칙',\n",
       " '간단',\n",
       " '은은',\n",
       " '답답',\n",
       " '탄탄',\n",
       " '저렴',\n",
       " '풍부',\n",
       " '궁금',\n",
       " '보송',\n",
       " '상쾌',\n",
       " '신선',\n",
       " '깨끗',\n",
       " '적당',\n",
       " '신기',\n",
       " '매끈',\n",
       " '자세',\n",
       " '충분',\n",
       " '찝찝',\n",
       " '푸석',\n",
       " '넉넉',\n",
       " '묵직',\n",
       " '비롯',\n",
       " '풍성',\n",
       " '말끔',\n",
       " '달콤',\n",
       " '화사',\n",
       " '웬만',\n",
       " '되직',\n",
       " '뿌듯',\n",
       " '익숙',\n",
       " '이만',\n",
       " '무방',\n",
       " '기특',\n",
       " '갑갑',\n",
       " '솔직',\n",
       " '강렬',\n",
       " '당연',\n",
       " '무난',\n",
       " '소중',\n",
       " '탁월',\n",
       " '활발',\n",
       " '어마어마',\n",
       " '훌륭',\n",
       " '쾌적',\n",
       " '용이',\n",
       " '든든',\n",
       " '꾸준',\n",
       " '빵빵',\n",
       " '흐뭇',\n",
       " '어둑',\n",
       " '도톰',\n",
       " '개운',\n",
       " '섬세',\n",
       " '지루',\n",
       " '비슷',\n",
       " '가뿐',\n",
       " '뽀송',\n",
       " '적절',\n",
       " '지만',\n",
       " '상세',\n",
       " '상이',\n",
       " '미지근',\n",
       " '만만',\n",
       " '마땅',\n",
       " '거뭇',\n",
       " '따뜻',\n",
       " '영롱',\n",
       " '중후',\n",
       " '혹독',\n",
       " '빠듯',\n",
       " '유연',\n",
       " '자잘',\n",
       " '빈번',\n",
       " '세세',\n",
       " '푸짐',\n",
       " '큼지막',\n",
       " '확실',\n",
       " '까칠',\n",
       " '까무잡잡',\n",
       " '따끈',\n",
       " '다채',\n",
       " '꿉꿉',\n",
       " '아찔',\n",
       " '복잡',\n",
       " '상큼',\n",
       " '빽빽',\n",
       " '지저분',\n",
       " '자글',\n",
       " '차분',\n",
       " '완연',\n",
       " '멀끔',\n",
       " '조밀',\n",
       " '딱딱',\n",
       " '죄송',\n",
       " '순박',\n",
       " '선선',\n",
       " '오묘',\n",
       " '생생',\n",
       " '깜찍']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_1 = df_dic[key_2]\n",
    "list(d_1[\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'맨 케어_Adj.xlsx'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_3 = 0\n",
    "key_2 = list(df_dic.keys())[i_3]\n",
    "key_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_2 = r\"C:\\Users\\Samsung\\Desktop\\Project\\이희정 교수님\\project\\data\\result\\result_long\\most_long\\most_tf_idf\\TFIDF_{}\"\n",
    "d_1 = df_dic[key_2]\n",
    "TFIDF = []\n",
    "for i_4 in d_1.index:\n",
    "    t = d_1.loc[i_4,\"word\"]\n",
    "    tf = d_1.loc[i_4,\"count\"]\n",
    "    df = 0\n",
    "    n = len(df_dic.keys())\n",
    "    for key_3 in df_dic.keys():\n",
    "        #print(key_3)\n",
    "        d_2 = df_dic[key_3]\n",
    "        if t in list(d_2[\"word\"]):\n",
    "            df += 1\n",
    "        #print()\n",
    "    #print(t,df)\n",
    "    idf = log(n/(df + 1))\n",
    "    tfidf = tf*idf\n",
    "    TFIDF.append(tfidf)\n",
    "d_1[\"TFIDF\"] = TFIDF\n",
    "d_1.to_excel(save_path_2.format(key_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|████████████                                                                        | 1/7 [00:03<00:18,  3.10s/it]\u001b[A\n",
      " 29%|████████████████████████                                                            | 2/7 [00:11<00:23,  4.75s/it]\u001b[A\n",
      " 43%|████████████████████████████████████                                                | 3/7 [00:27<00:32,  8.09s/it]\u001b[A\n",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [00:34<00:23,  7.87s/it]\u001b[A\n",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [00:51<00:20, 10.48s/it]\u001b[A\n",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [01:00<00:10, 10.16s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [01:12<00:00, 10.31s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "path_ = r\"C:\\Users\\Samsung\\Desktop\\Project\\이희정 교수님\\project\\data\\result\\result_long\\most_long\\most_topic\\{}\"\n",
    "\n",
    "\n",
    "for i_2 in tqdm(range(len(tf_dic.keys()))):\n",
    "    save_path_2 = r\"C:\\Users\\Samsung\\Desktop\\Project\\이희정 교수님\\project\\data\\result\\result_long\\most_long\\most_tf_idf\\TFIDF_{}\"\n",
    "    key_ = list(tf_dic.keys())[i_2]\n",
    "    df_dic = {}\n",
    "    for file in tf_dic[key_]:\n",
    "        #print(file)\n",
    "        df_ = load_data_v2(file).drop(columns=[\"setence_tag\",\"setence\",\"word2vec\"])\n",
    "        df_dic[file] = df_\n",
    "\n",
    "    for i_3 in range(len(df_dic.keys())):\n",
    "        key_2 = list(df_dic.keys())[i_3]\n",
    "        #print(key_2)\n",
    "        d_1 = df_dic[key_2]\n",
    "        TFIDF = []\n",
    "        for i_4 in d_1.index:\n",
    "            t = d_1.loc[i_4,\"word\"]\n",
    "            tf = d_1.loc[i_4,\"count\"]\n",
    "            df = 0\n",
    "            n = len(df_dic.keys())\n",
    "            for key_3 in df_dic.keys():\n",
    "                #print(key_3)\n",
    "                d_2 = df_dic[key_3]\n",
    "                if t in list(d_2[\"word\"]):\n",
    "                    df += 1\n",
    "                #print()\n",
    "            #print(t,df)\n",
    "            idf = log(n/(df + 1))\n",
    "            tfidf = tf*idf\n",
    "            TFIDF.append(tfidf)\n",
    "        d_1[\"TFIDF\"] = TFIDF\n",
    "        d_1.to_excel(save_path_2.format(key_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF 덮어 쓰기\n",
    "\n",
    "- 값이 0이 나온것: idf = (log(10/9 +1) 일때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [03:37<00:00, 31.01s/it]\n"
     ]
    }
   ],
   "source": [
    "dir_path_ = r\"C:\\Users\\Samsung\\Desktop\\Project\\이희정 교수님\\project\\data\\result\\result_long\\most_long\\most_topic\\LDA_{}\"\n",
    "save_path_2 = r\"C:\\Users\\Samsung\\Desktop\\Project\\이희정 교수님\\project\\data\\result\\result_long\\most_long\\most_tf_idf\\TFIDF_{}\"\n",
    "\n",
    "for i_2 in tqdm(range(len(tf_dic.keys()))):\n",
    "    key_ = list(tf_dic.keys())[i_2]\n",
    "    df_dic = {}\n",
    "    for file in tf_dic[key_]:\n",
    "        #print(file)\n",
    "        df_ = load_data_v2(file,dir_path_)#.drop(columns=[\"setence_tag\",\"setence\",\"word2vec\"])\n",
    "        df_dic[file] = df_\n",
    "\n",
    "    for i_3 in range(len(df_dic.keys())):\n",
    "        key_2 = list(df_dic.keys())[i_3]\n",
    "        #print(key_2)\n",
    "        d_1 = df_dic[key_2]\n",
    "        TFIDF = []\n",
    "        IDF = []\n",
    "        DF = []\n",
    "        for i_4 in d_1.index:\n",
    "            t = d_1.loc[i_4,\"word\"]\n",
    "            tf = d_1.loc[i_4,\"count\"]\n",
    "            df = 0\n",
    "            n = len(df_dic.keys())\n",
    "            for key_3 in df_dic.keys():\n",
    "                #print(key_3)\n",
    "                d_2 = df_dic[key_3]\n",
    "                if t in list(d_2[\"word\"]):\n",
    "                    df += 1\n",
    "                #print()\n",
    "            #print(t,df)\n",
    "            idf = log(n/(df + 1))\n",
    "            tfidf = tf*idf\n",
    "            TFIDF.append(tfidf)\n",
    "            IDF.append(idf)\n",
    "            DF.append(df)\n",
    "        d_1[\"TFIDF\"] = TFIDF\n",
    "        d_1.to_excel(save_path_2.format(key_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_2 = r\"C:\\Users\\Samsung\\Desktop\\Project\\TFIDF_{}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Noun'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_ = list(tf_dic.keys())[4]\n",
    "key_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dic = {}\n",
    "for file in tf_dic[key_]:\n",
    "    #print(file)\n",
    "    df_ = load_data_v2(file,dir_path_)#.drop(columns=[\"setence_tag\",\"setence\",\"word2vec\"])\n",
    "    df_dic[file] = df_\n",
    "\n",
    "for i_3 in range(len(df_dic.keys())):\n",
    "    key_2 = list(df_dic.keys())[i_3]\n",
    "    #print(key_2)\n",
    "    d_1 = df_dic[key_2]\n",
    "    TFIDF = []\n",
    "    IDF = []\n",
    "    DF = []\n",
    "    for i_4 in d_1.index:\n",
    "        t = d_1.loc[i_4,\"word\"]\n",
    "        tf = d_1.loc[i_4,\"count\"]\n",
    "        df = 0\n",
    "        n = len(df_dic.keys())\n",
    "        for key_3 in df_dic.keys():\n",
    "            #print(key_3)\n",
    "            d_2 = df_dic[key_3]\n",
    "            if t in list(d_2[\"word\"]):\n",
    "                df += 1\n",
    "            #print()\n",
    "        #print(t,df)\n",
    "        idf = log(n/(df + 1))\n",
    "        tfidf = tf*idf\n",
    "        TFIDF.append(tfidf)\n",
    "        IDF.append(idf)\n",
    "        DF.append(df)\n",
    "    d_1[\"TFIDF\"] = TFIDF\n",
    "    d_1[\"IDF\"] = IDF\n",
    "    d_1[\"DF\"] = DF\n",
    "    d_1.to_excel(save_path_2.format(key_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
