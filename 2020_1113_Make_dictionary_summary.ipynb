{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(r\"C:\\Users\\Samsung\\Desktop\\Project\\이희정 교수님\\project\\data\")\n",
    "file_list_xlsx = [f for f in file_list if f.endswith(\".xlsx\")]\n",
    "file_naver = [f for f in file_list_xlsx if f.endswith(\"네이버.xlsx\")]\n",
    "file_insta = [f for f in file_list_xlsx if f.endswith(\"인스타그램.xlsx\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dic 에 있는 file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"네이버\"\n",
    "\n",
    "def file_extraction(f_list,version):\n",
    "    pattern = r\"\\d* ([\\w|\\s]*)_{}.xlsx\".format(version)\n",
    "    #print(pattern)\n",
    "    reg = re.compile(pattern)\n",
    "    list_ = []\n",
    "    for file in f_list:\n",
    "        search = reg.search(file)\n",
    "        try:\n",
    "            target = search.group(1)\n",
    "            list_.append(target)\n",
    "        except:\n",
    "            pass\n",
    "    return list_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "taget_ = '맨 케어'\n",
    "path_ = r\"C:\\Users\\Samsung\\Desktop\\Project\\이희정 교수님\\project\\data\\{}\"\n",
    "\n",
    "def loading_data(target_,version,file_list,path_):\n",
    "    pat = r\".*{}_{}\".format(target_,version)\n",
    "    reg_2 = re.compile(pat)\n",
    "    file_ = list(filter(reg_2.match,file_list))[0]\n",
    "    data_ = pd.ExcelFile(path_.format(file_))\n",
    "    return data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat data sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_sheet(excel):\n",
    "    sheet_list = excel.sheet_names\n",
    "    df_ = pd.DataFrame()\n",
    "    for sheet in sheet_list:\n",
    "        sheet_data = excel.parse(sheet,encoding='cp949' )\n",
    "        sheet_data[\"Sheet\"] = sheet\n",
    "        df_ = pd.concat([df_,sheet_data]).reset_index(drop=True)\n",
    "    if \"naver_blog_review\" in df_.columns:\n",
    "        df_ = df_.rename(columns={\"naver_blog_review\":\"review\"})\n",
    "        df_[\"From\"] = \"Naver\"\n",
    "    else:\n",
    "        df_ = df_.rename(columns={\"insta_review\":\"review\"})\n",
    "        df_[\"From\"] = \"Insta\"\n",
    "    return df_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat naver & insta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_na_ins(target_,file_list):\n",
    "    ver_ = \"네이버\"\n",
    "    ta = target_\n",
    "    \n",
    "    ver_ = \"네이버\"\n",
    "    naver_ = loading_data(ta,ver_,file_list)\n",
    "    a = concat_sheet(naver_)\n",
    "\n",
    "    ver_ = \"인스타그램\"\n",
    "    insta_ = loading_data(ta,ver_,file_list)\n",
    "    b = concat_sheet(insta_)\n",
    "    df_naver = a.loc[:,[\"제품명(영문)\",\"review\",\"Sheet\",\"From\"]]\n",
    "    df_insta = b.loc[:,[\"제품명(영문)\",\"review\",\"Sheet\",\"From\"]]\n",
    "    df_all = pd.concat([df_naver,df_insta]).reset_index(drop=True)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'맨 케어'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ver_ = \"네이버\"\n",
    "ta = file_extraction(file_list,ver_)[1]\n",
    "ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = concat_na_ins(ta,file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(df_):\n",
    "    pos = []\n",
    "    words = []\n",
    "    #sentences = []\n",
    "    for i in tqdm(range(len(df_))):\n",
    "        try:\n",
    "            p = kkma.pos(df_[\"review\"][i])\n",
    "            #senten = kkma.sentences(df_[\"review\"][i])\n",
    "        except:\n",
    "            text = df_[\"review\"][i] + \"']\"\n",
    "            #senten = []\n",
    "            try:\n",
    "                p = kkma.pos(text)\n",
    "            except:\n",
    "                p = []\n",
    "        word = [w[0] for w in p]\n",
    "        pos.append(p)\n",
    "        words.append(word)\n",
    "        #sentences.append(senten)\n",
    "    df_[\"tag\"] = pos\n",
    "    df_[\"words\"] = words\n",
    "    #df_[\"sentences\"] = sentences\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging_long(df_):\n",
    "    pos = []\n",
    "    words = []\n",
    "    sentences = []\n",
    "    for i in tqdm(range(len(df_))):\n",
    "        try:\n",
    "            p = kkma.pos(df_[\"review\"][i])\n",
    "            senten = kkma.sentences(df_[\"review\"][i])\n",
    "        except:\n",
    "            text = df_[\"review\"][i] + \"']\"\n",
    "            senten = []\n",
    "            try:\n",
    "                p = kkma.pos(text)\n",
    "            except:\n",
    "                p = []\n",
    "        word = [w[0] for w in p]\n",
    "        pos.append(p)\n",
    "        words.append(word)\n",
    "        sentences.append(senten)\n",
    "    df_[\"tag\"] = pos\n",
    "    df_[\"words\"] = words\n",
    "    df_[\"sentences\"] = sentences\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_a[0:10]\n",
    "df_tag = pos_tagging(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tag별 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_tag.copy()\n",
    "tag_dic = {\n",
    "    \"Noun\":[\"NNG\",\"NNP\",\"NNM\"],\n",
    "    \"Verb\":['VV', 'VA', 'VXV', 'VCN'],\n",
    "    \"Adverb\":[\"MAG\",\"MAC\"],\n",
    "    \"Adj\":[\"XPV\",\"XSM\",\"XSO\",\"XR\"],\n",
    "    \"Unknown\":['UN', 'UV', 'UE'],\n",
    "    \"ETC\":[\"EMO\",\"OL\"],\n",
    "    \"All\" : [\"NNG\",\"NNP\",\"NNM\",'VV', 'VA', 'VXV', 'VCN',\"MAG\",\"MAC\",\"XPV\",\"XSM\",\"XSO\",\"XR\",'UN', 'UV', 'UE',\"EMO\",\"OL\"]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def tag_split(df,tag_dic):\n",
    "    def only_(df,igs):\n",
    "        list_1 = []\n",
    "        for list_ in df.loc[:,\"tag\"]:\n",
    "            list_2 = []\n",
    "            for i in range(len(list_)):\n",
    "                #print(list_[i])\n",
    "                if list_[i][1] in igs:\n",
    "                    list_2.append(list_[i][0])\n",
    "            list_1.append(list_2)\n",
    "        return list_1\n",
    "\n",
    "    def ignore_(df,igs):\n",
    "        list_1 = []\n",
    "        for list_ in df.loc[:,\"tag\"]:\n",
    "            list_2 = list_.copy()\n",
    "            for i in range(len(list_)):\n",
    "                #print(list_[i])\n",
    "                if list_[i][1] in igs:\n",
    "                    list_2.remove(list_[i])\n",
    "            list_3 = [w[0] for w in list_2]\n",
    "            list_1.append(list_3)\n",
    "        return list_1\n",
    "    df.loc[:,\"Noun\"] = only_(df,tag_dic[\"Noun\"])\n",
    "    df.loc[:,\"Verb\"] = only_(df,tag_dic[\"Verb\"])\n",
    "    df.loc[:,\"Adverb\"] = only_(df,tag_dic[\"Adverb\"])\n",
    "    df.loc[:,\"Adj\"] = only_(df,tag_dic[\"Adj\"])\n",
    "    df.loc[:,\"Unknown\"] = only_(df,tag_dic[\"Unknown\"])\n",
    "    df.loc[:,\"ETC\"] = only_(df,tag_dic[\"ETC\"])\n",
    "    df.loc[:,\"All\"] = only_(df,tag_dic[\"All\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = tag_split(df_tag,tag_dic)\n",
    "df_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec 학습기 ( 나누기 전에)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all = df_split\n",
    "win_size = 5\n",
    "target_col = \"All\"\n",
    "\n",
    "def word2vec_model(df_all,target_col,win_size):\n",
    "    model_words = Word2Vec(sentences=df_all[target_col], window = win_size, min_count=0,iter=100, sg=1)\n",
    "    return model_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word = word2vec_model(df_split,\"All\",5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make_most df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Noun'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_list = list(tag_dic.keys())\n",
    "target_tag = target_list[0]\n",
    "target_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_n = 30\n",
    "#df_ = df_split\n",
    "\n",
    "def make_mostdf(df_,target_tag,most_n):\n",
    "    df_most = pd.DataFrame()\n",
    "    word = Counter(df_[target_tag].sum()).most_common(most_n)\n",
    "    df_most[\"word\"] = [n[0] for n in word]\n",
    "    df_most[\"count\"] = [n[1] for n in word]\n",
    "    return df_most\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = make_mostdf(df_split,target_tag,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해당 문장 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_split\n",
    "df_m = dm.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>제품명(영문)</th>\n",
       "      <th>review</th>\n",
       "      <th>Sheet</th>\n",
       "      <th>From</th>\n",
       "      <th>tag</th>\n",
       "      <th>words</th>\n",
       "      <th>sentences</th>\n",
       "      <th>Noun</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Adverb</th>\n",
       "      <th>Adj</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>ETC</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Super Moisture Gel</td>\n",
       "      <td>[]</td>\n",
       "      <td>01 기초 스킨 케어</td>\n",
       "      <td>Naver</td>\n",
       "      <td>[([, SS), (], SS)]</td>\n",
       "      <td>[[, ]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SIGNATURE ALL IN ONE ESSENCE MOISTURE PLUS</td>\n",
       "      <td>['끈적임없는 남자에센스추천 포맨트 시그니처 올인원 모이스처 플러스         ...</td>\n",
       "      <td>01 기초 스킨 케어</td>\n",
       "      <td>Naver</td>\n",
       "      <td>[([, SS), (', SS), (끈적이, VV), (ㅁ, ETN), (없, VA...</td>\n",
       "      <td>[[, ', 끈적이, ㅁ, 없, 는, 남자, 에, 센스, 추천, 포, 맨트, 시, ...</td>\n",
       "      <td>[[' 끈적임 없는 남자에 센스 추천 포 맨트 시 그니 처 올인원 모이 스처 플러스...</td>\n",
       "      <td>[남자, 센스, 추천, 포, 시, 처, 올인원, 모이, 플러스, 안녕, 뷰, 스타,...</td>\n",
       "      <td>[끈적이, 없, 그, 아니, 갖, 블, 그, 두, 그리하, 되, 하, 바르, 바쁘,...</td>\n",
       "      <td>[안, 늘, 또, 안, 다, 없이, 다, 특히나, 더, 많이, 잘, 너무나, 꽤, ...</td>\n",
       "      <td>[민감, 촉촉, 산뜻, 답답, 갑갑, 촉촉, 깔끔, 촉촉, 답답, 흐뭇, 산뜻, 산...</td>\n",
       "      <td>[맨트, 스처, 린, 트시, 스처, 있고보습과, 맨트, 스처, 준비해야겠더라고, 맨...</td>\n",
       "      <td>[ㅋㅋ, ml, ㅎㅎ, ^^;, ㅋㅋ, ㅠ_ㅠ, ㅋㅋ, ^^;;, ml, ㅋㅋ, ^...</td>\n",
       "      <td>[끈적이, 없, 남자, 센스, 추천, 포, 맨트, 시, 그, 처, 올인원, 모이, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SIGNATURE ALL IN ONE ESSENCE</td>\n",
       "      <td>['끈적임없는 남자에센스추천 포맨트 시그니처 올인원 모이스처 플러스         ...</td>\n",
       "      <td>01 기초 스킨 케어</td>\n",
       "      <td>Naver</td>\n",
       "      <td>[([, SS), (', SS), (끈적이, VV), (ㅁ, ETN), (없, VA...</td>\n",
       "      <td>[[, ', 끈적이, ㅁ, 없, 는, 남자, 에, 센스, 추천, 포, 맨트, 시, ...</td>\n",
       "      <td>[[' 끈적임 없는 남자에 센스 추천 포 맨트 시 그니 처 올인원 모이 스처 플러스...</td>\n",
       "      <td>[남자, 센스, 추천, 포, 시, 처, 올인원, 모이, 플러스, 안녕, 뷰, 스타,...</td>\n",
       "      <td>[끈적이, 없, 그, 아니, 갖, 블, 그, 두, 그리하, 되, 하, 바르, 바쁘,...</td>\n",
       "      <td>[안, 늘, 또, 안, 다, 없이, 다, 특히나, 더, 많이, 잘, 너무나, 꽤, ...</td>\n",
       "      <td>[민감, 촉촉, 산뜻, 답답, 갑갑, 촉촉, 깔끔, 촉촉, 답답, 간단, 산뜻, 어...</td>\n",
       "      <td>[맨트, 스처, 린, 트시, 스처, 있고보습과, 맨트, 스처, 맨트, 스처, 스처,...</td>\n",
       "      <td>[ㅋㅋ, ml, ㅎㅎ, ml, ㅎㅎ, ml, ml, ^^, ㅠㅠ, ㅠㅠ, ㅠㅠ, ㅠ...</td>\n",
       "      <td>[끈적이, 없, 남자, 센스, 추천, 포, 맨트, 시, 그, 처, 올인원, 모이, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AQUAPOWER CLEAR ESSENCE</td>\n",
       "      <td>['남자스킨 써보니 역시! 아쿠아파워 올인원 비오템 옴므         안녕하세요!...</td>\n",
       "      <td>01 기초 스킨 케어</td>\n",
       "      <td>Naver</td>\n",
       "      <td>[([, SS), (', SS), (남자, NNG), (스킨, NNG), (쓰, V...</td>\n",
       "      <td>[[, ', 남자, 스킨, 쓰, 어, 보, 니, 역시, !, 아쿠아, 파워, 올인원...</td>\n",
       "      <td>[[' 남자 스킨 써 보니 역시! 아쿠아 파워 올인원 비오 템 옴 므 안녕하세요!,...</td>\n",
       "      <td>[남자, 스킨, 아쿠아, 파워, 올인원, 비, 오, 옴, 안녕, 패션, 뷰티, 로,...</td>\n",
       "      <td>[쓰, 보, 블, 그, 빠르, 다가오, 느껴지, 춥, 쉽, 쉽, 던지, 하, 아니,...</td>\n",
       "      <td>[역시, 부쩍, 이렇게, 가장, 특히나, 특히, 물론, 아무래도, 조금, 더, 가장...</td>\n",
       "      <td>[강력, 자세, 궁금, 풍부, 촉촉, 은은, 촉촉, 간편, 촉촉, 풍부, 이만, 풍...</td>\n",
       "      <td>[므, 피부, 그루밍제품, 그루밍, 옴므라, 틱, 틱, 컨셉, 비오템옴므, 쥰, 비...</td>\n",
       "      <td>[BIOTHERM, HOMME, Needs, T-PUR, T-PUR, BIOTHER...</td>\n",
       "      <td>[남자, 스킨, 쓰, 보, 역시, 아쿠아, 파워, 올인원, 비, 오, 옴, 므, 안...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FORCE SUPREME DUAL AMPOULE</td>\n",
       "      <td>['여자들만큼 남자들도 피부에 따라서 나이가 달라보이고 전체적인 느낌이 달라지잖아요...</td>\n",
       "      <td>01 기초 스킨 케어</td>\n",
       "      <td>Naver</td>\n",
       "      <td>[([, SS), (', SS), (여자, NNG), (들, XSN), (만큼, N...</td>\n",
       "      <td>[[, ', 여자, 들, 만큼, 남자, 들, 도, 피부, 에, 따르, 아서, 나이,...</td>\n",
       "      <td>[[' 여자들 만큼 남자들도 피부에 따라서 나이가 달라 보이고 전체적인 느낌이 달라...</td>\n",
       "      <td>[여자, 만큼, 남자, 피부, 나이, 전체적, 느낌, 만큼, 오빠, 동생, 애인, ...</td>\n",
       "      <td>[따르, 닿, 보이, 달라지, 그러, 있, 있, 드리, 저, 잦, 피곤하, 보이, ...</td>\n",
       "      <td>[마침, 점점, 따로, 너무, 바로, 정말, 많이, 잘, 잘, 그리고, 더, 지금,...</td>\n",
       "      <td>[탄탄, 칙칙, 강력, 탄탄, 강력, 강력, 강렬, 시원, 시원, 산뜻, 산뜻, 푸...</td>\n",
       "      <td>[앰플, 므, 앰플, 프트, 므, 앰플, 이스, 기프트박스, 므, 앰플, 닝, 앰플...</td>\n",
       "      <td>[ml, ml, ml, ml, ^^, C, C, ㅎㅎ, ㅎㅎ, CLICK, ^^, ...</td>\n",
       "      <td>[여자, 만큼, 남자, 피부, 따르, 나이, 닿, 보이, 전체적, 느낌, 달라지, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      제품명(영문)  \\\n",
       "0                          Super Moisture Gel   \n",
       "1  SIGNATURE ALL IN ONE ESSENCE MOISTURE PLUS   \n",
       "2                SIGNATURE ALL IN ONE ESSENCE   \n",
       "3                     AQUAPOWER CLEAR ESSENCE   \n",
       "4                  FORCE SUPREME DUAL AMPOULE   \n",
       "\n",
       "                                              review        Sheet   From  \\\n",
       "0                                                 []  01 기초 스킨 케어  Naver   \n",
       "1  ['끈적임없는 남자에센스추천 포맨트 시그니처 올인원 모이스처 플러스         ...  01 기초 스킨 케어  Naver   \n",
       "2  ['끈적임없는 남자에센스추천 포맨트 시그니처 올인원 모이스처 플러스         ...  01 기초 스킨 케어  Naver   \n",
       "3  ['남자스킨 써보니 역시! 아쿠아파워 올인원 비오템 옴므         안녕하세요!...  01 기초 스킨 케어  Naver   \n",
       "4  ['여자들만큼 남자들도 피부에 따라서 나이가 달라보이고 전체적인 느낌이 달라지잖아요...  01 기초 스킨 케어  Naver   \n",
       "\n",
       "                                                 tag  \\\n",
       "0                                 [([, SS), (], SS)]   \n",
       "1  [([, SS), (', SS), (끈적이, VV), (ㅁ, ETN), (없, VA...   \n",
       "2  [([, SS), (', SS), (끈적이, VV), (ㅁ, ETN), (없, VA...   \n",
       "3  [([, SS), (', SS), (남자, NNG), (스킨, NNG), (쓰, V...   \n",
       "4  [([, SS), (', SS), (여자, NNG), (들, XSN), (만큼, N...   \n",
       "\n",
       "                                               words  \\\n",
       "0                                             [[, ]]   \n",
       "1  [[, ', 끈적이, ㅁ, 없, 는, 남자, 에, 센스, 추천, 포, 맨트, 시, ...   \n",
       "2  [[, ', 끈적이, ㅁ, 없, 는, 남자, 에, 센스, 추천, 포, 맨트, 시, ...   \n",
       "3  [[, ', 남자, 스킨, 쓰, 어, 보, 니, 역시, !, 아쿠아, 파워, 올인원...   \n",
       "4  [[, ', 여자, 들, 만큼, 남자, 들, 도, 피부, 에, 따르, 아서, 나이,...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0                                               [[]]   \n",
       "1  [[' 끈적임 없는 남자에 센스 추천 포 맨트 시 그니 처 올인원 모이 스처 플러스...   \n",
       "2  [[' 끈적임 없는 남자에 센스 추천 포 맨트 시 그니 처 올인원 모이 스처 플러스...   \n",
       "3  [[' 남자 스킨 써 보니 역시! 아쿠아 파워 올인원 비오 템 옴 므 안녕하세요!,...   \n",
       "4  [[' 여자들 만큼 남자들도 피부에 따라서 나이가 달라 보이고 전체적인 느낌이 달라...   \n",
       "\n",
       "                                                Noun  \\\n",
       "0                                                 []   \n",
       "1  [남자, 센스, 추천, 포, 시, 처, 올인원, 모이, 플러스, 안녕, 뷰, 스타,...   \n",
       "2  [남자, 센스, 추천, 포, 시, 처, 올인원, 모이, 플러스, 안녕, 뷰, 스타,...   \n",
       "3  [남자, 스킨, 아쿠아, 파워, 올인원, 비, 오, 옴, 안녕, 패션, 뷰티, 로,...   \n",
       "4  [여자, 만큼, 남자, 피부, 나이, 전체적, 느낌, 만큼, 오빠, 동생, 애인, ...   \n",
       "\n",
       "                                                Verb  \\\n",
       "0                                                 []   \n",
       "1  [끈적이, 없, 그, 아니, 갖, 블, 그, 두, 그리하, 되, 하, 바르, 바쁘,...   \n",
       "2  [끈적이, 없, 그, 아니, 갖, 블, 그, 두, 그리하, 되, 하, 바르, 바쁘,...   \n",
       "3  [쓰, 보, 블, 그, 빠르, 다가오, 느껴지, 춥, 쉽, 쉽, 던지, 하, 아니,...   \n",
       "4  [따르, 닿, 보이, 달라지, 그러, 있, 있, 드리, 저, 잦, 피곤하, 보이, ...   \n",
       "\n",
       "                                              Adverb  \\\n",
       "0                                                 []   \n",
       "1  [안, 늘, 또, 안, 다, 없이, 다, 특히나, 더, 많이, 잘, 너무나, 꽤, ...   \n",
       "2  [안, 늘, 또, 안, 다, 없이, 다, 특히나, 더, 많이, 잘, 너무나, 꽤, ...   \n",
       "3  [역시, 부쩍, 이렇게, 가장, 특히나, 특히, 물론, 아무래도, 조금, 더, 가장...   \n",
       "4  [마침, 점점, 따로, 너무, 바로, 정말, 많이, 잘, 잘, 그리고, 더, 지금,...   \n",
       "\n",
       "                                                 Adj  \\\n",
       "0                                                 []   \n",
       "1  [민감, 촉촉, 산뜻, 답답, 갑갑, 촉촉, 깔끔, 촉촉, 답답, 흐뭇, 산뜻, 산...   \n",
       "2  [민감, 촉촉, 산뜻, 답답, 갑갑, 촉촉, 깔끔, 촉촉, 답답, 간단, 산뜻, 어...   \n",
       "3  [강력, 자세, 궁금, 풍부, 촉촉, 은은, 촉촉, 간편, 촉촉, 풍부, 이만, 풍...   \n",
       "4  [탄탄, 칙칙, 강력, 탄탄, 강력, 강력, 강렬, 시원, 시원, 산뜻, 산뜻, 푸...   \n",
       "\n",
       "                                             Unknown  \\\n",
       "0                                                 []   \n",
       "1  [맨트, 스처, 린, 트시, 스처, 있고보습과, 맨트, 스처, 준비해야겠더라고, 맨...   \n",
       "2  [맨트, 스처, 린, 트시, 스처, 있고보습과, 맨트, 스처, 맨트, 스처, 스처,...   \n",
       "3  [므, 피부, 그루밍제품, 그루밍, 옴므라, 틱, 틱, 컨셉, 비오템옴므, 쥰, 비...   \n",
       "4  [앰플, 므, 앰플, 프트, 므, 앰플, 이스, 기프트박스, 므, 앰플, 닝, 앰플...   \n",
       "\n",
       "                                                 ETC  \\\n",
       "0                                                 []   \n",
       "1  [ㅋㅋ, ml, ㅎㅎ, ^^;, ㅋㅋ, ㅠ_ㅠ, ㅋㅋ, ^^;;, ml, ㅋㅋ, ^...   \n",
       "2  [ㅋㅋ, ml, ㅎㅎ, ml, ㅎㅎ, ml, ml, ^^, ㅠㅠ, ㅠㅠ, ㅠㅠ, ㅠ...   \n",
       "3  [BIOTHERM, HOMME, Needs, T-PUR, T-PUR, BIOTHER...   \n",
       "4  [ml, ml, ml, ml, ^^, C, C, ㅎㅎ, ㅎㅎ, CLICK, ^^, ...   \n",
       "\n",
       "                                                 All  \n",
       "0                                                 []  \n",
       "1  [끈적이, 없, 남자, 센스, 추천, 포, 맨트, 시, 그, 처, 올인원, 모이, ...  \n",
       "2  [끈적이, 없, 남자, 센스, 추천, 포, 맨트, 시, 그, 처, 올인원, 모이, ...  \n",
       "3  [남자, 스킨, 쓰, 보, 역시, 아쿠아, 파워, 올인원, 비, 오, 옴, 므, 안...  \n",
       "4  [여자, 만큼, 남자, 피부, 따르, 나이, 닿, 보이, 전체적, 느낌, 달라지, ...  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_sentence_long(df_,df_m):\n",
    "    exts = []\n",
    "    for tar in df_m[\"word\"]:\n",
    "        target = tar\n",
    "        extraction = []\n",
    "        for sentences in df_[\"sentences\"]:\n",
    "            for senten in sentences:\n",
    "                if target in senten:\n",
    "                    extraction.append(senten)\n",
    "\n",
    "        exts.append(extraction)\n",
    "    df_m[\"setence\"] = exts\n",
    "    return df_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "def find_sentence_short(df_,df_m,window_size):\n",
    "    exts = []\n",
    "    for tar in df_m[\"word\"]:\n",
    "        target = tar\n",
    "        extraction = []\n",
    "        for i in df_[\"review\"]:\n",
    "            sentences = i.split(\"  \")\n",
    "            for senten in sentences:\n",
    "                if target in senten:\n",
    "                    index_ = senten.find(target)\n",
    "                    #extraction.append(senten)\n",
    "                    try:\n",
    "                        extraction.append(senten[index_-window_size:index_+window_size])\n",
    "                    except:\n",
    "                        pass\n",
    "        exts.append(extraction)\n",
    "    df_m[\"setence\"] = exts\n",
    "    return df_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_split\n",
    "df_m = dm.copy()\n",
    "\n",
    "df_m = find_sentence_short(df_split,dm,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec similar 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(df_,w2_model):\n",
    "    most_simi = []\n",
    "    for i in df_[\"word\"]:\n",
    "        ms = w2_model.wv.most_similar(i,topn=20)\n",
    "        most_simi.append(ms)\n",
    "    df_[\"word2vec\"] = most_simi\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ = dm\n",
    "w2_model = model_word\n",
    "\n",
    "df_m = most_similar(dm,model_word)\n",
    "df_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ver_ = \"네이버\"\n",
    "ta = file_extraction(file_list,ver_)[1]\n",
    "ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = concat_na_ins(ta,file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_a[0:10]\n",
    "df_tag = pos_tagging(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = tag_split(df_tag,tag_dic)\n",
    "df_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word = word2vec_model(df_split,\"All\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = list(tag_dic.keys())\n",
    "target_tag = target_list[0]\n",
    "target_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dm = make_mostdf(df_split,target_tag,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_split\n",
    "df_m = dm.copy()\n",
    "\n",
    "df_m = find_sentence_short(df_split,dm,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ = dm\n",
    "w2_model = model_word\n",
    "\n",
    "df_m = most_similar(dm,model_word)\n",
    "df_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
